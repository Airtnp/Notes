# OS Idioms

## \#



## A
* Actor
* + non-blocking IO + IO multiplexing
* + Event demultiplexor
* + [ref](https://www.zhihu.com/question/26943938)
* + [ref](http://www.yeolar.com/note/2012/12/15/high-performance-io-design-patterns/)
* + Reactor
* + - epoll
* + Proactor
* + - IOCP

## B
* backpointer-based consistency (BBC)
* block corruption
* + checksum
* + - xor-based
* + - CRC (cyclic redundancy check)


## C
* conditional variable
* + wait
* + signal/signal_all
* + Mesa semantics (always while loops)
* crash consistency
* + data block (write / update)
* + inode (write / update)
* + bitmap (write / update)
* copy-on-write
* cache
* + performance
* + - hit time
* + - - small and simple cache
* + - - way prediction
* + - - trace cache
* + - cache bandwidth
* + - - nonblocking
* + - - multibanked
* + - - pipelined
* + - - multi-level cache
* + - - [ref](https://www.quora.com/What-is-meant-by-non-blocking-cache-and-multi-banked-cache)
* + - miss penalty
* + - - critical word first
* + - - merging write buffers
* + - miss rate
* + - - compiler optimization
* + - via parallelism
* + - - hardware prefetching
* + - - compiler prefetching
* cache consistency
* + update visibiility
* + stale cache
* + ping-pong cache
* + - same data
* + false-sharing
* + - data stored in same cacheline
* + transparent to programmer
* + MESI
* concurrency
* + MPI
* + OpenMP
* + oversubscription
* Critical section
* + Dekker
* + Peterson
* + Lamport Bakery
* + Szymanski
* + Eisenberg & McGuire


## D
* Dining Philosophers
* + global order acquire lock
* + global servicer
* + Chandy/Misra
* DMA
* Device driver
* disk scheduler
* + SSTF
* + - only seek time (move between tracks)
* + Elevator(SCAN/C-SCAN)
* + - [ref](http://www.cnblogs.com/jianyungsun/archive/2011/03/16/1986439.html)
* + SPTF/SATF
* + - seek + rotation time
* distributed system
* + reliable communication layers
* + - sender ->(meesage) receiver ->(ack) -> sender
* + - timeout -> dropped request / ack
* + DSM (distributed shared memory)
* + RPC (remote procedure call)
* + - stub generator (protocol compiler)
* + - - client stub
* + - - - create message buffer
* + - - - pack information
* + - - - send message to destination RPC server
* + - - - wait for reply
* + - - - unpack result
* + - - - return to caller
* + - - server stub
* + - - - unpack (unmarshaling/deserialization)
* + - - - call info actual function
* + - - - pack result
* + - - - send reply
* + - run-time library
* + - - naming
* + - - fragmentation
* + - - reassembly
* + - - byte ordering
* + - - - XDR (external data representation) / protobuf
* + - - synchronously
* + [AFS-NFS-GFS](http://www.shuang0420.com/2016/12/10/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/)
* + NFS
* + - [ref](https://csruiliu.github.io/blog/2017/11/17/nfs/)
* + - sharing
* + - centralized administration
* + - security
* + - simple and fast server crash recovery
* + - stateless protocol
* + - - shared state / distributed state -> complicating crash recovery
* + - idempotent opeartion
* + - client side caching
* + - flush on close
* + - block-aligned r/w
* + AFS
* + - [ref](https://csruiliu.github.io/blog/2017/11/17/afs/)
* + - [ref](http://blog.csdn.net/ak913/article/details/7197062)
* + - callback system
* + - polling
* + - last writer(closer) wins
* + GFS
* + - [ref](http://www.d-kai.me/google-file-system/)



## E
* Event-driven system
* + asynchronous IO
* + continuation based
* + signal/interrupt -> check complete
* + multi-core paging
* ECC (error correcting code)


## F
* futex
* + 2-phase lock
* + futex_wait/futex_wake
* file system
* + FAT (file allocation table)
* + NTFS
* + ext2-4
* + - super (metadata) + bitmap (data/inode) + inode + data
* + UFS (FFS)
* + - [ref](http://www.d-kai.me/ffs-unix%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%BC%BB%E7%A5%96/)
* + - increment block size
* + - fragment (subblock)
* + - block group (superblock, inode/data block) / Allocation group
* + - [ref](https://csruiliu.github.io/blog/2017/11/18/ffs/)
* + LFS (log-structured file system) / WAFL / ZFS / btrfs
* + - [ref](https://csruiliu.github.io/blog/2017/11/19/lfs/)
* + - reason
* + - - memory size ++
* + - - random IO << sequential IO
* + - - other poorly on common wordloads
* + - - not RAID-aware
* + - segment only with update
* + - write-buffering
* + - imap (inode map)
* + - shadow paging update
* + ZFS
* + - [ref](https://www.ibm.com/developerworks/cn/linux/l-zfs/index.html)
* + - [doc](https://docs.oracle.com/cd/E19253-01/819-7065/index.html)
* + - [ref](http://blog.csdn.net/wdy_yx/article/details/42848773)
* FSCK (file system checker)
* + check superblock
* + check inodes (free blocks) + inode state + inode link
* + duplicates
* + bad blocks
* + directory checks


## G


## H
* Hard disk
* + SAS (Serial Attached SCSI)
* + SATA (Serial ATA)
* + [ref](https://www.webopedia.com/DidYouKnow/Computer_Science/sas_sata.asp)
* hypervisors
* hazard pointer
* + [ref](http://blog.csdn.net/coryxie/article/details/8562627)


## I



## J
* journaling (write-ahead logging)
* + TID (transaction id)
* + data write
* + journal write (TxB metadata data)
* + journal commit (TxE)
* + checkpoint (write on-disk)
* + free journal

## K


## L
* LSE (latent-sector errors) / URE (unrecoverable read error)
* + RAID-DP
* lost write
* + write verify (read-after write)
* + ZFS checksum
* lock
* + mutex
* + shared_mutex (reader-write lock)
* + - upgrade_mutex
* + timed_mutex
* + recursive_mutex
* + deadlock
* + - bounded resources
* + - no preemption
* + - wait while holding
* + - circular waiting (not DAG)
* + livelock: two thread restart each other
* lock-free
* + test-and-set 
* + compare-and-swap (CAS)
* + - DWCAS
* + fetch-and-add (tick lock)
* + load-linked/store-conditional MIPS/ARM
* + - will cause spurious failure
* + data structures
* + - [ref](https://liblfds.org/mediawiki/index.php?title=White_Papers)
* + ABA
* + - x = A -> B -> A => transparent to CAS
* + - add ABA counter


## M
* mutex
* misdirected write
* + PID (physical id)
* monitor
* + [ref](https://www.jianshu.com/p/8b3ed769bc9f)
* + Mesa
* + Hoare


## N



## O
* order / cache consistency / memory coherence
* + reorder
* + - compiler reorder
* + - - `__asm__ __volatile__ (""::: "memory")` ensure compiler on sequence-before order
* + - CPU reorder
* + - - X86: `__asm__ __volatile__ ("mfence"::: "memory")` (lfence/sfence) ensure CPU on happens-before order
* + multicore cache protocal (MESI)
* + [ref](https://www.zhihu.com/question/24301047)
* + memory order <m (不同内存的读/写 != CPU执行 != 程序顺序(program order <p)不同)
* + - R(L) -> R
* + - R -> W(S)
* + - W -> R
* + - - store buffer waiting for response, so reorder faster
* + - W -> W
* + - 同一内存 atomic_operation => sequenced-before
* + - 单核(Logic Core) => transparent
* + - 不同内存 atomic_operation => 乱序
* + Relaxed
* + - TSO (total store order): relax W -> R [x86]
* + - PSO (partial store order): relax W -> W
* + - Weak ordering / Power PC / Release: relax R -> W / R -> R
* + sequenced-before => single thread happens-before
* + happens-before
* + - inter-thread happens-before
* + - - A synchronizes-with B
* + - - A is dependency-ordered before B
* + - - A synchronizes-with some evaluation X, and X is sequenced-before B
* + - - A is sequenced-before some evaluation X, and X inter-thread happens-before B
* + - - A inter-thread happens-before some evaluation X, and X inter-thread happens-before B
* + - sequenced-befire
* + - dependency-ordered-before
* + synchronized-with
* + visible side-effects
* + - happens-before
* + - no middle other side effect
* + seq_cst: SC
* + - every atomic operation is not reordered between thread
* + - Atomic operations tagged memory_order_seq_cst not only order memory the same way as release/acquire ordering (everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load), but also establish a single total modification order of all atomic operations that are so tagged.
* + relaxed: Relaxed
* + - atomic on specific variable not reordered in single thread
* + - Atomic operations tagged memory_order_relaxed are not synchronization operations; they do not impose an order among concurrent memory accesses. They only guarantee atomicity and modification order consistency.
* + acq_rel/consume/acquire/release: Acquire-Release
* + - release synchronized-with acquire
* + - side effect: write before release visible to read after acquire
* + - If an atomic store in thread A is tagged memory_order_release and an atomic load in thread B from the same variable is tagged memory_order_acquire, all memory writes (non-atomic and relaxed atomic) that happened-before the atomic store from the point of view of thread A, become visible side-effects in thread B, that is, once the atomic load is completed, thread B is guaranteed to see everything thread A wrote to memory.
* + consume: dependency-ordered-before
* + - release synchronized-with consume
* + - If an atomic store in thread A is tagged memory_order_release and an atomic load in thread B from the same variable is tagged memory_order_consume, all memory writes (non-atomic and relaxed atomic) that are dependency-ordered-before the atomic store from the point of view of thread A, become visible side-effects within those operations in thread B into which the load operation carries dependency, that is, once the atomic load is completed, those operators and functions in thread B that use the value obtained from the load are guaranteed to see what thread A wrote to memory.
* + memory_order_relaxed	
* + - Relaxed operation: there are no synchronization or ordering constraints imposed on other reads or writes, only this operation's atomicity is guaranteed (see Relaxed ordering below)
* + memory_order_consume	
* + - A load operation with this memory order performs a consume operation on the affected memory location: no reads or writes in the current thread dependent on the value currently loaded can be reordered before this load. Writes to data-dependent variables in other threads that release the same atomic variable are visible in the current thread. On most platforms, this affects compiler optimizations only (see Release-Consume ordering below)
* + memory_order_acquire	
* + - A load operation with this memory order performs the acquire operation on the affected memory location: no reads or writes in the current thread can be reordered before this load. All writes in other threads that release the same atomic variable are visible in the current thread (see Release-Acquire ordering below)
* + - Ensure LL/LS (RR/RW)
* + memory_order_release	
* + - A store operation with this memory order performs the release operation: no reads or writes in the current thread can be reordered after this store. All writes in the current thread are visible in other threads that acquire the same atomic variable (see Release-Acquire ordering below) and writes that carry a dependency into the atomic variable become visible in other threads that consume the same atomic (see Release-Consume ordering below).
* + - Ensure LS/SS (RW/WW)
* + memory_order_acq_rel	
* + - A read-modify-write operation with this memory order is both an acquire operation and a release operation. No memory reads or writes in the current thread can be reordered before or after this store. All writes in other threads that release the same atomic variable are visible before the modification and the modification is visible in other threads that acquire the same atomic variable.
* + memory_order_seq_cst	
* + - A load operation with this memory order performs an acquire operation, a store performs a release operation, and read-modify-write performs both an acquire operation and a release operation, plus a single total order exists in which all threads observe all modifications in the same order (see Sequentially-consistent ordering below)
* + atomic variable-variable or fence-fence
* + lock ensures read-acquire + write-release (lock-acquire / unlock-release) (AKA memory fence)



## P
* preemptive
* PIO (programmed IO)
* parallelism


## Q



## R
* RAID
* + [ref](https://zh.wikipedia.org/wiki/RAID)
* + RAID0: Striping
* + RAID1: Mirroring
* + RAID4: Saving space with parity
* + RAID5: rotating parity
* RCU (read-copy-update)
* RAM
* + SRAM
* + - cache
* + DRAM
* + EEPROM
* + Flash
* + ECC
* + Odd-Even check bit
* + Chipkill
* 


## S
* scheduler
* + affinity
* + Round-robin
* + SJF STCF
* + MLFQ (Windows NT)
* + multi-core
* + - SQMS
* + - MQMS
* + - O(1)
* + - CFS
* + - BFS
* spinlock / lock-free
* + test-and-set 
* + compare-and-swap (CAS)
* + - DWCAS
* + fetch-and-add (tick lock)
* + load-linked/store-conditional MIPS/ARM
* + - will cause spurious failure
* semaphore
* + sem_wait (-1 wait if negative otherwise run)
* + sem_post (+1 wake if 1 or more waiting)
* + 
* soft-update

## T
* TLB
* + hardware-handling
* + software-handling 
* + - [ref](https://en.wikipedia.org/wiki/Translation_lookaside_buffer#TLB-miss_handling)
* + - [another-ref-says-RISC-and-IA-64](http://www.informit.com/articles/article.aspx?p=29961&seqNum=4)
* + PCID/ASID/PID


## U

## V
* Virtualization
* + CPU
* + - scheduler
* + - limited direct execution
* + Memory
* + - address translation
* + - segmentation
* + - virtual <=> physical
* + - - paging
* + - - TLB
* + VMM
* + I/O
* VMM (virutal machine monitor)
* + [ref](http://blog.csdn.net/flyforfreedom2008/article/details/45113635)
* + CPU: limited direct execution
* + - machine switch
* + - privileged operation
* + - trap => OS trap trampoline
* + Memory
* + - VM page table => VPN-to-MFN
* + - software handled TLB: VM TLB handler => OS TLB handler
* + - hardware handled TLB: shadow page table
* + information gap
* + - idle loop
* + - zeroing of page table
* + I/O
* + para-virtualization


## W



## X



## Y



## Z

